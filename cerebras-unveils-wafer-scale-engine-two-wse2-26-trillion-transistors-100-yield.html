<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:site_name" content="ZBlogL"><meta property="og:type" content="article"><meta name=robots content="index,follow,noarchive"><meta property="og:image" content="//img/home-bg-jeep.jpg"><meta property="twitter:image" content="//img/home-bg-jeep.jpg"><meta name=title content="2.6 Trillion Transistors, 100% Yield"><meta property="og:title" content="2.6 Trillion Transistors, 100% Yield"><meta property="twitter:title" content="2.6 Trillion Transistors, 100% Yield"><meta name=description content="The last few years has seen a glut of processors enter the market with the sole purpose of accelerating artificial intelligence and machine learning workloads. Due to the different types of machine learning algorithms possible, these processors are often focused on a few key areas, but one thing limits them all  how big you"><meta property="og:description" content="The last few years has seen a glut of processors enter the market with the sole purpose of accelerating artificial intelligence and machine learning workloads. Due to the different types of machine learning algorithms possible, these processors are often focused on a few key areas, but one thing limits them all  how big you"><meta property="twitter:description" content="The last few years has seen a glut of processors enter the market with the sole purpose of accelerating artificial intelligence and machine learning workloads. Due to the different types of machine learning algorithms possible, these processors are often focused on a few key areas, but one thing limits them all  how big you"><meta property="twitter:card" content="summary"><meta name=keyword content><link rel="shortcut icon" href=./img/favicon.ico><title>2.6 Trillion Transistors, 100% Yield |</title><link rel=canonical href=./cerebras-unveils-wafer-scale-engine-two-wse2-26-trillion-transistors-100-yield.html><link rel=stylesheet href=https://assets.cdnweb.info/hugo/cleanwhite/css/bootstrap.min.css><link rel=stylesheet href=https://assets.cdnweb.info/hugo/cleanwhite/css/hugo-theme-cleanwhite.min.css><link rel=stylesheet href=https://assets.cdnweb.info/hugo/cleanwhite/css/zanshang.css><link href=https://cdn.jsdelivr.net/gh/FortAwesome/Font-Awesome@5.15.1/css/all.css rel=stylesheet type=text/css><script src=https://assets.cdnweb.info/hugo/cleanwhite/js/jquery.min.js></script>
<script src=https://assets.cdnweb.info/hugo/cleanwhite/js/bootstrap.min.js></script>
<script src=https://assets.cdnweb.info/hugo/cleanwhite/js/hux-blog.min.js></script></head><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container-fluid><div class="navbar-header page-scroll"><button type=button class=navbar-toggle>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span></button>
<a class=navbar-brand href=./>ZBlogL</a></div><div id=huxblog_navbar><div class=navbar-collapse><ul class="nav navbar-nav navbar-right"><li><a href=./categories/blog>blog</a></li><li><a href=./sitemap.xml>Sitemap</a></li><li><a href=./index.xml>RSS</a></li></ul></div></div></div></nav><script>var $body=document.body,$toggle=document.querySelector(".navbar-toggle"),$navbar=document.querySelector("#huxblog_navbar"),$collapse=document.querySelector(".navbar-collapse");$toggle.addEventListener("click",handleMagic);function handleMagic(){$navbar.className.indexOf("in")>0?($navbar.className=" ",setTimeout(function(){$navbar.className.indexOf("in")<0&&($collapse.style.height="0px")},400)):($collapse.style.height="auto",$navbar.className+=" in")}</script><style type=text/css>header.intro-header{background-image:url(/img/home-bg-jeep.jpg)}</style><header class=intro-header><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><div class=tags></div><h1>2.6 Trillion Transistors, 100% Yield</h1><h2 class=subheading></h2><span class=meta>Posted by
Reinaldo Massengill
on
Thursday, September 5, 2024</span></div></div></div></div></header><article><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
post-container"><p>The last few years has seen a glut of processors enter the market with the sole purpose of accelerating artificial intelligence and machine learning workloads. Due to the different types of machine learning algorithms possible, these processors are often focused on a few key areas, but one thing limits them all – how big you can make the processor. Two years ago Cerebras unveiled a revolution in silicon design: a processor as big as your head, using as much area on a 12-inch wafer as a rectangular design would allow, built on 16nm, focused on both AI as well as HPC workloads. Today the company is launching its second generation product, built on TSMC 7nm, with more than double the cores and more than double of everything.</p><h2>Second Generation Wafer Scale Engine</h2><p>The new processor from Cerebras builds on the first by moving to TSMC’s N7 process. This allows the logic to scale down, as well as to some extent the SRAMs, and now the new chip has 850,000 AI cores on board. Basically almost everything about the new chip is over 2x:</p><table border=0 width=85%><tbody><tr class=tgrey><td colspan=4>Cerebras Wafer Scale</td></tr><tr class=tlblue><td><i>AnandTech</i></td><td>Wafer Scale<br>Engine Gen1</td><td>Wafer Scale<br>Engine Gen2</td><td>Increase</td></tr><tr><td class=tlgrey><strong>AI Cores</strong></td><td>400,000</td><td>850,000</td><td>2.13x</td></tr><tr><td class=tlgrey><strong>Manufacturing</strong></td><td>TSMC 16nm</td><td>TSMC 7nm</td><td>-</td></tr><tr><td class=tlgrey><strong>Launch Date</strong></td><td>August 2019</td><td>Q3 2021</td><td>-</td></tr><tr><td class=tlgrey><strong>Die Size</strong></td><td>46225 mm2</td><td>46225 mm2</td><td>-</td></tr><tr><td class=tlgrey><strong>Transistors</strong></td><td>1200 billion</td><td>2600 billion</td><td>2.17x</td></tr><tr><td class=tlgrey><strong>(Density)</strong></td><td>25.96 mTr/mm2</td><td>56.246 mTr/mm2</td><td>2.17x</td></tr><tr><td class=tlgrey><strong>On-board SRAM</strong></td><td>18 GB</td><td>40 GB</td><td>2.22x</td></tr><tr><td class=tlgrey><strong>Memory Bandwidth</strong></td><td>9 PB/s</td><td>20 PB/s</td><td>2.22x</td></tr><tr><td class=tlgrey><strong>Fabric Bandwidth</strong></td><td>100 Pb/s</td><td>220 Pb/s</td><td>2.22x</td></tr><tr><td class=tlgrey><strong>Cost</strong></td><td>$2 million+</td><td>arm+leg</td><td>‽</td></tr></tbody></table><p>As with the original processor, known as the Wafer Scale Engine (WSE-1), the new WSE-2 features hundreds of thousands of AI cores across a massive 46225 mm2 of silicon. &nbsp;In that space, Cerebras has enabled 2.6 trillion transistors for 850,000 cores - by comparison, the second biggest AI CPU on the market is ~826 mm2, with 0.054 trillion transistors. Cerebras also cites 1000x more onboard memory, with 40 GB of SRAM, compared to 40 MB on the Ampere A100.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/16626/fvds_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a><br>Me with Wafer Scale Gen1 - looks the same, but with less than half the cores.</p><p>The cores are connected with a 2D Mesh with FMAC datapaths. Cerebras achieves 100% yield by designing a system in which any manufacturing defect can be bypassed – initially Cerebras had 1.5% extra cores to allow for defects, but we’ve since been told this was way too much as TSMC's process is so mature. Cerebras’ goal with WSE is to provide a single platform, designed through innovative patents, that allowed for bigger processors useful in AI calculations but has also been extended into a wider array of HPC workloads.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/16626/202008182321081_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><h3>Building on First Gen WSE</h3><p>A key to the design is the custom graph compiler, that takes pyTorch or TensorFlow and maps each layer to a physical part of the chip, allowing for asynchronous compute as the data flows through. Having such a large processor means the data never has to go off-die and wait in memory, wasting power, and can continually be moved onto the next stage of the calculation in a pipelined fashion. The compiler and processor are also designed with sparsity in mind, allowing high utilization regardless of batch size, or can enable parameter search algorithms to run simultaneously.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/16626/Cerebras%20WSE2%20Launch%20Embargoed%20until%204%2020%2021%5B2%5D%5B2%5D%5B3%5D%5B3%5D%5B4%5D%5B1%5D-page-006_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>For Cerebras’ first generation WSE is sold as a complete system called CS-1, and the company has several dozen customers with deployed systems up and running, including a number of research laboratories, pharmaceutical companies, biotechnology research, military, and the oil and gas industries. Lawrence Livermore has a CS-1 paired to its 23 PFLOP ‘Lassen’ Supercomputer. Pittsburgh Supercomputer Center purchased two systems with a $5m grant, and these systems are attached to their Neocortex supercomputer, allowing for simultaneous AI and enhanced compute.</p><h3>Products and Partnerships</h3><p>Cerebras sells complete CS-1 systems today as a 15U box that contains one WSE-1 along with 12x100 GbE, twelve 4 kW power supplies (6 redundant, peak power about 23 kW), and deployments at some institutions are paired with HPE’s SuperDome Flex. The new CS-2 system shares this same configuration, albeit with more than double the cores and double the on-board memory, but still within the same power. Compared to other platforms, these processors are arranged vertically inside the 15U design in order to enable ease of access as well as built-in liquid cooling across such a large processor. It should also be noted that those front doors are machined from a single piece of aluminium.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/16626/Cerebras%20WSE2%20Launch%20Embargoed%20until%204%2020%2021%5B2%5D%5B2%5D%5B3%5D%5B3%5D%5B4%5D%5B1%5D-page-016_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>The uniqueness of Cerebras’ design is being able to go beyond the physical manufacturing limits normally presented in manufacturing, known as the reticle limit. Processors are designed with this limit as the maximum size of a chip, as connecting two areas with a cross-reticle connection is difficult. This is part of the secret sauce that Cerebras brings to the table, and the company remains the only one offering a processor on this scale – the same patents that Cerebras developed and were awarded to build these large chips are still in play here, and the second gen WSE will be built into CS-2 systems with a similar design to CS-1 in terms of connectivity and visuals.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/16626/Cerebras%20WSE2%20Launch%20Embargoed%20until%204%2020%2021%5B2%5D%5B2%5D%5B3%5D%5B3%5D%5B4%5D%5B1%5D-page-017_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>The same compiler and software packages with updates enable any customer that has been trialling AI workloads with the first system to use the second at the point at which they deploy one. Cerebras has been working on higher-level implementations to enable customers with standardized TensorFlow and PyTorch models very quick assimilation of their existing GPU code by adding three lines of code and using Cerebras’ graph compiler. The compiler then divides the whole 850,000 cores into segments of each layer that allow for data flow in a pipelined fashion without stalls. The silicon can also be used for multiple networks simultaneously for parameter search.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/16626/Cerebras%20WSE2%20Launch%20Embargoed%20until%204%2020%2021%5B2%5D%5B2%5D%5B3%5D%5B3%5D%5B4%5D%5B1%5D-page-019_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>Cerebras states that with having such a large single chip solution means that the barrier to distributed training methods across 100s of AI chips is now so much further away that this excess complication is not needed in most scenarios – to that, we’re seeing CS-1 deployments of single systems attached to supercomputers. However, Cerebras is keen to point out that two CS-2 systems will deliver 1.7 million AI cores in a standard 42U rack, or three systems for 2.55 million in a larger 46U rack (assuming there’s sufficient power for all at once!), replacing a dozen racks of alternative compute hardware. <a href=#>At Hot Chips 2020</a>, Chief Hardware Architect Sean Lie stated that one of Cerebras' key benefits to customers was the ability to enable workload simplification that previously required racks of GPU/TPU but instead can run on a single WSE in a computationally relevant fashion.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/16626/Cerebras%20WSE2%20Launch%20Embargoed%20until%204%2020%2021%5B2%5D%5B2%5D%5B3%5D%5B3%5D%5B4%5D%5B1%5D-page-024_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>As a company, Cerebras has ~300 staff across Toronto, San Diego, Tokyo, and San Francisco. They have dozens of customers already with CS-1 deployed and a number more already trialling CS-2 remotely as they bring up the commercial systems. Beyond AI, Cerebras is getting a lot of interest from typical commercial high performance compute markets, such as oil-and-gas and genomics, due to the flexibility of the chip is enabling fluid dynamics and other compute simulations. Deployments of CS-2 will occur later this year in Q3, and the price has risen from ~$2-3 million to ‘several’ million.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/16626/CS21_7nm_Scale_Godzilla_sml%20%281%29_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a><br>With Godzilla for a size reference</p><h3>Related Reading</h3><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmivp6x7orrAp5utnZOde6S7zGiqoaenZH53gpFvZpydopqvs63SZqynrpWeubR51pqdnqpdqLCiuMRmnKefmaOybsDWqGSwq5VnenOCjK2poqScnryvedOrmKermajBsL7SZmhpaF2utqa4ww%3D%3D</p><hr><ul class=pager><li class=previous><a href=./barbara-rush.html data-toggle=tooltip data-placement=top title="Barbara Rush">&larr;
Previous Post</a></li><li class=next><a href=./suleka-mathew-net-worth.html data-toggle=tooltip data-placement=top title="Suleka Mathew Net Worth">Next
Post &rarr;</a></li></ul></div><div class="col-lg-2 col-lg-offset-0
visible-lg-block
sidebar-container
catalog-container"><div class=side-catalog><hr class="hidden-sm hidden-xs"><h5><a class=catalog-toggle href=#>CATALOG</a></h5><ul class=catalog-body></ul></div></div><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
sidebar-container"><section><hr class="hidden-sm hidden-xs"><h5><a href=./tags/>FEATURED TAGS</a></h5><div class=tags></div></section></div></div></div></article><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"></ul><p class="copyright text-muted">Copyright &copy; ZBlogL 2024<br><a href=https://themes.gohugo.io/hugo-theme-cleanwhite>CleanWhite Hugo Theme</a> |
<iframe style=margin-left:2px;margin-bottom:-5px frameborder=0 scrolling=0 width=100px height=20px src="https://ghbtns.com/github-btn.html?user=zhaohuabing&repo=hugo-theme-cleanwhite&type=star&count=true"></iframe></p></div></div></div></footer><script>function loadAsync(i,t){var n=document,s="script",e=n.createElement(s),o=n.getElementsByTagName(s)[0];e.src=i,t&&e.addEventListener("load",function(e){t(null,e)},!1),o.parentNode.insertBefore(e,o)}</script><script>$("#tag_cloud").length!==0&&loadAsync("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:"#bbbbee",end:"#0085a1"}},$("#tag_cloud a").tagcloud()})</script><script>loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js",function(){var e=document.querySelector("nav");e&&FastClick.attach(e)})</script><script type=text/javascript>function generateCatalog(e){_containerSelector="div.post-container";var t,n,s,o,i,r=$(_containerSelector),a=r.find("h1,h2,h3,h4,h5,h6");return $(e).html(''),a.each(function(){n=$(this).prop("tagName").toLowerCase(),i="#"+$(this).prop("id"),s=$(this).text(),t=$('<a href="'+i+'" rel="nofollow">'+s+"</a>"),o=$('<li class="'+n+'_nav"></li>').append(t),$(e).append(o)}),!0}generateCatalog(".catalog-body"),$(".catalog-toggle").click(function(e){e.preventDefault(),$(".side-catalog").toggleClass("fold")}),loadAsync("https://assets.cdnweb.info/hugo/cleanwhite/js/jquery.nav.js",function(){$(".catalog-body").onePageNav({currentClass:"active",changeHash:!1,easing:"swing",filter:"",scrollSpeed:700,scrollOffset:0,scrollThreshold:.2,begin:null,end:null,scrollChange:null,padding:80})})</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/banner.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>